{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = center> 100 Days of Machine Learning - Day 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 days of machine learning is a tech challenge where the participants spend 100 continuous days studying, learning and coding machine learning concepts. It involves dedicating a certain amount of time each day to engage in ML-related activities, such as reading books, watching tutorials, completing online courses, working on projects, or participating in coding exercises. The goal is to develop a consistent learning habit and make significant progress in ML skills over the course of 100 days."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- Feature Selection\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score\n",
    "- Precision vs Recall Tradeoff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Any real-life dataset comprises of various features and fields that describe the relation of an entity with another. In most datasets, specially big ones, we observe that not all features or fields are strongly correlated (positively or negatively) with each other or atleast with the quantity we are trying to measure or predict (the target variable). If we include all the features present in a dataset, it poses a couple of problems :\n",
    "\n",
    "- High Dimensionality : Dealing with a large number of features also means that the dimensionality of the dataset is increased. A dataset being \"high-dimensional\" means that there are a large number of input features and variables that the model has to consider, analyse and optimize its predictions according to. When a model becomes overly high dimensional, the model becomes incredibly complex and computationaly expensive. Therefore it is important to only select the important features that are correlated with the target variable.\n",
    "\n",
    "    A frequently observed instance of high dimensional data is image data. Images consist of hundreds of pixels and each pixel can be termed as an input feature. Processing all the data for a final model can be very computationally complex, hence it is important to detect and seperate features from the images first using techniques like filters.\n",
    "\n",
    "- Model Performance : If a dataset contains many features, and an amount of those features are poorly correlated with the target variable or contain noise/faulty data- then the model trained on the dataset can be subject to performance problems like overfitting, which occurs when the model learns irrelevant noise and patterns that lead to poor generalization to unseen data but very good performance on training data.\n",
    "\n",
    "    Selecting specific features which are relevant, mathematically correlated or appropriate for the training of the model reduces the amount of noise and faults present in the input dataset. This reduces overfitting and betters the performance of the model.\n",
    "\n",
    "    Moreover, if the data is extremely high-dimensional because of a large amount of features, the computational complexity of the model increases exponentially. This causes training of the model to slow down and make it harder for iterative learning to occur. This means a lot of features that may or may not be interrelated with each other have to be computed and with each added data point, the computational cost of resolving the distance, relation or densities increases.\n",
    "\n",
    "- Interpretability and Complexity : A high amount of features make a model very complex because of high dimensionality, which also means it may be difficult to interpret and understand the model's behaviour. Most unmodified and uncleaned datasets contain irrelevant features and noise, which when used to train a model may result in misleading information as the model becomes more and more susceptible to overfitting.\n",
    "\n",
    "    When we do feature selection, we understand directly what features are correlated with the target variable and how they may be related to it. By isolating the relevant features we gain more insight into how the model is internally operating, making the black box system of an ML model more tranparent and easier to interpret. \n",
    "\n",
    "    Since feature selection reduces complexity, representing the understood workings of a model also becomes easier and more simplified. This allows for easier and intuitive explanations and understanding of models.\n",
    "\n",
    "    If there is no feature selection done on the input dataset, it will often lead to *a loss in detail*, which means we lose the understanding of what features may be directly correlated with the target variable(s). This can cause a model to lose its attention to the important features and also the interpretability of the model to decrease because of potential misinformation gain."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods in feature selection\n",
    "\n",
    "There are several feature selection methods that can be employed for various scenarios in machine learning. Feature selection aims to find the best representative features for a given problem or for dealing with a target variable. Feature selection techniques exist for both supervised and unsupervised methods.\n",
    "\n",
    "## Filter Method\n",
    "\n",
    "Filter methods are series of statistically based feature selection methods which guage the relevance of a feature based on their individual characteristics or statistical properties. They are computationally efficient methods that can be used while preprocessing the data.\n",
    "\n",
    "There's various types of filter methods: \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
