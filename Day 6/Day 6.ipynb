{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = center> 100 Days of Machine Learning - Day 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 days of machine learning is a tech challenge where the participants spend 100 continuous days studying, learning and coding machine learning concepts. It involves dedicating a certain amount of time each day to engage in ML-related activities, such as reading books, watching tutorials, completing online courses, working on projects, or participating in coding exercises. The goal is to develop a consistent learning habit and make significant progress in ML skills over the course of 100 days."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- Feature Selection\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score\n",
    "- Precision vs Recall Tradeoff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Any real-life dataset comprises of various features and fields that describe the relation of an entity with another. In most datasets, specially big ones, we observe that not all features or fields are strongly correlated (positively or negatively) with each other or atleast with the quantity we are trying to measure or predict (the target variable). If we include all the features present in a dataset, it poses a couple of problems :\n",
    "\n",
    "- High Dimensionality : Dealing with a large number of features also means that the dimensionality of the dataset is increased. A dataset being \"high-dimensional\" means that there are a large number of input features and variables that the model has to consider, analyse and optimize its predictions according to. When a model becomes overly high dimensional, the model becomes incredibly complex and computationaly expensive. Therefore it is important to only select the important features that are correlated with the target variable.\n",
    "\n",
    "    A frequently observed instance of high dimensional data is image data. Images consist of hundreds of pixels and each pixel can be termed as an input feature. Processing all the data for a final model can be very computationally complex, hence it is important to detect and seperate features from the images first using techniques like filters.\n",
    "\n",
    "- Model Performance : If a dataset contains many features, and an amount of those features are poorly correlated with the target variable or contain noise/faulty data- then the model trained on the dataset can be subject to performance problems like overfitting, which occurs when the model learns irrelevant noise and patterns that lead to poor generalization to unseen data but very good performance on training data.\n",
    "\n",
    "    Selecting specific features which are relevant, mathematically correlated or appropriate for the training of the model reduces the amount of noise and faults present in the input dataset. This reduces overfitting and betters the performance of the model.\n",
    "\n",
    "    Moreover, if the data is extremely high-dimensional because of a large amount of features, the computational complexity of the model increases exponentially. This causes training of the model to slow down and make it harder for iterative learning to occur. This means a lot of features that may or may not be interrelated with each other have to be computed and with each added data point, the computational cost of resolving the distance, relation or densities increases.\n",
    "\n",
    "- Interpretability and Complexity : A high amount of features make a model very complex because of high dimensionality, which also means it may be difficult to interpret and understand the model's behaviour. Most unmodified and uncleaned datasets contain irrelevant features and noise, which when used to train a model may result in misleading information as the model becomes more and more susceptible to overfitting.\n",
    "\n",
    "    When we do feature selection, we understand directly what features are correlated with the target variable and how they may be related to it. By isolating the relevant features we gain more insight into how the model is internally operating, making the black box system of an ML model more tranparent and easier to interpret. \n",
    "\n",
    "    Since feature selection reduces complexity, representing the understood workings of a model also becomes easier and more simplified. This allows for easier and intuitive explanations and understanding of models.\n",
    "\n",
    "    If there is no feature selection done on the input dataset, it will often lead to *a loss in detail*, which means we lose the understanding of what features may be directly correlated with the target variable(s). This can cause a model to lose its attention to the important features and also the interpretability of the model to decrease because of potential misinformation gain."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods in feature selection\n",
    "\n",
    "There are several feature selection methods that can be employed for various scenarios in machine learning. Feature selection aims to find the best representative features for a given problem or for dealing with a target variable. Feature selection techniques exist for both supervised and unsupervised methods.\n",
    "\n",
    "## Filter Methods\n",
    "\n",
    "Filter methods are series of statistically based feature selection methods which guage the relevance of a feature based on their individual characteristics or statistical properties. They are computationally efficient methods that can be used while preprocessing the data.\n",
    "\n",
    "There's various types of filter methods:\n",
    "\n",
    "## Correlation methods\n",
    "This method measures the correlation between the target variable and the features. This method was explored on [Day 2](https://github.com/snowclipsed/100daysofml/blob/main/Day%202/Day%202.ipynb) of 100DoMLC. \n",
    "\n",
    "--- \n",
    "### Information Gain Method\n",
    "\n",
    "Information Gain is a method of calculating the reduction of entropy from feature transformation, i.e. ; it tells us the amount of informationt hat a feature provides about a target variable. Entropy is lowered because it is the loss of information and information from a featuee is gained. Information in the context of a machine learning dataset can also be termed as how \"surprising\" an encountered data point is to the model - if it is less surprising or is a lower probability event, the model learns that what it knows stands, if it is a lot more surprising than expected or has a high probability to occur acc to the model, then the model may have to take some steps to adjust this newly gained information about the data point. \n",
    "\n",
    "Measuring the amount of information gained tells us how strongly a feature affects the target variable, specially in the context of decision tree algorithms, where a feature of high information gain is chosen as the feature to split for at each node.\n",
    "\n",
    "In a binary classification problem with an output of 0 and 1, we can measure entropy by the formula : $$ Entropy = -(p(0)) * log(P(0)) + p(1) * log(P(1)) $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a dataset has a perfectly balanced probability distribution, it has a high entropy because the surprise factor of getting any given value is the highest in such a distribution. However, if the probability distribution is skewed towards a direction, we understand that more values are concentrated towards the region of the graph where the probability is the highest."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information gain can use entropy to find out the purity or the skewedness of a dataset. A more pure dataset means that the probability distribution of the datapoints is more uniform.\n",
    "\n",
    "Suppose we have a dataset \"S\" and a random variable \"a\"\n",
    "\n",
    "then Information Gain :\n",
    "\n",
    "$$ IG (S,a) = H(S) - H(S | a) $$\n",
    "\n",
    "where IG(S,a) is the information gain from the dataset, and H(S) is the standard entropy of the unchanged model. Meanwhile, H(S | a) is the entropy with a variable \"a\" into consideration. H(S | a) can be calculated by taking the ratio of examples in the dataset where a variable a has a value V to the total datapoints in the dataset multiplied by the combined entropy of the examples where the variable a has value V.\n",
    "\n",
    "$$ H(S | a) = \\sum \\frac{Sa(v)}{S} * H(Sa(v)) $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Chi Squared Test Method\n",
    "\n",
    "This method is valid for selecting features which are categorical and for categorical datasets. We apply the Chi-Square method on each categorical feature and pick the features which have good chi squared scores. \n",
    "\n",
    "The Chi-Squared score is based on the null hypothesis, which assumes that any feature or variable has zero correlation with one another as well as the alternate hypothesis which means that a feature or variable has some amount of correlation with the target feature/variable. The Chi-square formula works on the concept of frequencies or occurances of datapoints in the dataset and work well with frequency based datasets.\n",
    "\n",
    "\n",
    "After definining our two hypotheses, we draw a contingency table with the observed frequencies/values of the features. Next, we find the expected value of the features according to our first hypothesis, the null hypothesis. \n",
    "\n",
    "$$ \n",
    "\n",
    "P(A \\cap B) = P(A) * P(B) \n",
    "$$\n",
    "or\n",
    "$$\n",
    "Expected Frequency = \\frac {Row Frequency  * Column Frequency }{GrandSum}\n",
    "$$ \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import a dataset to perform the chi-squared method on :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_data = pd.read_csv(\"data/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass   \n",
       "0          892         0       3  \\\n",
       "1          893         1       3   \n",
       "2          894         0       2   \n",
       "3          895         0       3   \n",
       "4          896         1       3   \n",
       "\n",
       "                                           Name     Sex   Age  SibSp  Parch   \n",
       "0                              Kelly, Mr. James    male  34.5      0      0  \\\n",
       "1              Wilkes, Mrs. James (Ellen Needs)  female  47.0      1      0   \n",
       "2                     Myles, Mr. Thomas Francis    male  62.0      0      0   \n",
       "3                              Wirz, Mr. Albert    male  27.0      0      0   \n",
       "4  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female  22.0      1      1   \n",
       "\n",
       "    Ticket     Fare Cabin Embarked  \n",
       "0   330911   7.8292   NaN        Q  \n",
       "1   363272   7.0000   NaN        S  \n",
       "2   240276   9.6875   NaN        Q  \n",
       "3   315154   8.6625   NaN        S  \n",
       "4  3101298  12.2875   NaN        S  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
       "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_first_saved = chi_data.loc[(chi_data['Pclass'] == 1) & (chi_data['Survived'] == 1)]\n",
    "p_first_lost = chi_data.loc[(chi_data['Pclass'] == 1) & (chi_data['Survived'] == 0)]\n",
    "p_second_saved = chi_data.loc[(chi_data['Pclass'] == 2) & (chi_data['Survived'] == 1)]\n",
    "p_second_lost = chi_data.loc[(chi_data['Pclass'] == 2) & (chi_data['Survived'] == 0)]\n",
    "p_third_saved = chi_data.loc[(chi_data['Pclass'] == 3) & (chi_data['Survived'] == 1)]\n",
    "p_third_lost = chi_data.loc[(chi_data['Pclass'] == 3) & (chi_data['Survived'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 57)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p_first_saved.index), len(p_first_lost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
