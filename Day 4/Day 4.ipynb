{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = center> 100 Days of Machine Learning - Day 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 days of machine learning is a tech challenge where the participants spend 100 continuous days studying, learning and coding machine learning concepts. It involves dedicating a certain amount of time each day to engage in ML-related activities, such as reading books, watching tutorials, completing online courses, working on projects, or participating in coding exercises. The goal is to develop a consistent learning habit and make significant progress in ML skills over the course of 100 days."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "\n",
    "Gradient Descent or the gradient descent function is an optimization algorithm that is used to minimize the value of a function, typically a loss function in a machine learning environment. This is achieved by the algorithm adjusting various parameters inside the model and helps to \"train\" the model.\n",
    "\n",
    "Gradient descent is called so because it does a stepwise decent on the cost function gradient towards the local or global minima.\n",
    "\n",
    "Gradient descent is not a single algorithm, it is a collection of algorithms that are used in different scenarios variatingly just like the cost function, but the basic principle of all gradient descent approaches is the same.\n",
    "\n",
    "Here's how gradient descent works in general : \n",
    "\n",
    " - Initialize the parameters of the model with some values.\n",
    " - Calculate gradient loss.\n",
    " - Update the current parameters by subtracting a small amount or a fractional amount of the gradient based on the learning rate.\n",
    " - Repeat till a minima or convergence is reached or a specific number of iterations takes place. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\n",
    "w = w - \\alpha \\frac{{d}}{{dx}} J(w,b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where Î± is the learning rate in the equation, J(w,b) is the cost function of f(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
