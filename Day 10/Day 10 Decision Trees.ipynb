{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = center> 100 Days of Machine Learning - Day 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 days of machine learning is a tech challenge where the participants spend 100 continuous days studying, learning and coding machine learning concepts. It involves dedicating a certain amount of time each day to engage in ML-related activities, such as reading books, watching tutorials, completing online courses, working on projects, or participating in coding exercises. The goal is to develop a consistent learning habit and make significant progress in ML skills over the course of 100 days."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Note : This notebook is continued from Day 9 </b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- Decision Trees\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision Trees (DTs) are supervised learning algorithms. They are used for classification as well as regression tasks. As their name suggests, Decision *Trees* have a tree-like structure which comprises of root node, branch nodes and leaf nodes. This structure is heirarichal in nature.\n",
    "\n",
    "<div align = \"center\"> <img src = \"https://www.mastersindatascience.org/wp-content/uploads/sites/54/2022/05/tree-graphic.jpg\" height = 500px> </div>\n",
    "\n",
    "The approach behind decision trees is basic but is really effective for a lot of scenarios. A decision tree splits a problem into a number of decisions based on the input features and the target variable. These splits are represented by the root node splitting into several branch nodes, and those branch nodes splitting even further till a leaf node or output is met. This is similar to a biological tree where water from the soil starts from the roots, is transported into either branches depending on where the water is needed, and ends up in a leaf for photosynthesis.\n",
    "\n",
    "## Components of a Decision Tree\n",
    "\n",
    "A decision tree has several components like Root node, Branches, Decision Nodes and Leaf nodes - let's discuss them one by one.\n",
    "\n",
    "- Root Node : A root node is the node/point of origin for the entire decision tree. A root node represents the entire population sample/dataset and the branch nodes originating from the root node then divide this dataset into 2 or more subsets.\n",
    "\n",
    "- Decision Node : A decision node is one that symbolizes a choice regarding a specific input feature or question asked. There may be two or more choice sprouting from such a node.\n",
    "\n",
    "- Branch : A branch is a section of a tree that is formed by \"splitting\" the tree from a decision node or a root node.\n",
    "\n",
    "- Leaf Node : Also called a \"terminal\" node, it is a node that does not have any child node ; i.e. ; it does not split into branches/further nodes. The leaf node represents a final outcome. \n",
    "\n",
    "\n",
    "Now, you can do various things with these components of decision trees : \n",
    "\n",
    "- Pruning : It refers to cutting down nodes/branches that do not provide any additional information or value for the purpose of preventing model overfitting.\n",
    "\n",
    "<div align = \"center\">\n",
    "\n",
    "<img src = \"https://upload.wikimedia.org/wikipedia/commons/2/23/Before_after_pruning.png\" height = 200px>\n",
    "\n",
    "</div>\n",
    "- Splitting : Splitting a decision tree refers to the process of creating branches or sub-trees from a decision node. The act of splitting is done depending on the features, dataset and problem at hand.\n",
    "\n",
    "<div align = \"center\">\n",
    "<img src = \"https://i.ytimg.com/vi/gqlWi4HjRYI/maxresdefault.jpg\" height = 300px>\n",
    "<div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that decision trees follow a [Sum-of-Product](https://www.electronics-tutorials.ws/boolean/sum-of-product.html) or Disjunctive Normal Form. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do Decision Trees work?\n",
    "\n",
    "The main motive of a decision tree classifier is to select which attributes or features it needs to make a decision upon, or make them a root node. There are several methods to do this and these are called \"Attribute Selection Measures\" or ASM. ASM can be understood as metrics that help define the usefulness of a feature while we split the dataset for decision-making. This is usually done by gauging the amount of raw information a feature provides in relation with a target variable. \n",
    "\n",
    "Decision trees aim to maximize the information gained while splitting the data - this means if the data is correctly split - the groups that the data is split into will become more and more homogenous. A decision tree algorithm repeats this process of splitting till an acceptable level of homogenity is reached allowing the algorithm to classify unknown data points into correct classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribute Selection Measures\n",
    "\n",
    "If we randomly start to pick features in a decision tree using a brute force approach, it would take a lot of time and iterations to find the right dataset split and nodes, possibly even resulting in bad accuracy. This is why we instead use attribute selection methods to find features to select as a decision node. ASM are a set of criteria that one can use to better identify the right features over iterations of building the decision tree.\n",
    "\n",
    "\n",
    "Such criteria are : \n",
    "\n",
    "- Entropy\n",
    "- Information Gain\n",
    "- Gini Index\n",
    "- Chi Square Method\n",
    "- Fisher's Index\n",
    "- Variance Reduction\n",
    "\n",
    "A lot of commonly used feature selection techniques are utilized to select attributes. These techniques are explained [here](https://github.com/snowclipsed/100daysofml/blob/main/Day%206/Day%206.ipynb)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss these concepts in relation to the decision tree method.\n",
    "\n",
    "\n",
    "### Entropy in Decision Trees\n",
    "\n",
    "Entropy can be called a measure of randomness in a dataset or subset of the dataset. When we are talking about entropy in decision trees, it refers to how randomly spread classes are inside the data. Therefore it allows us to understand if a split made by the decision tree algorithm is \"pure\" or not. Purity of a split refers to how homogenous the split occuring is and how uncertain it is while decision making. \n",
    "\n",
    "\n",
    "In the ID3 decision tree algorithm, any decision tree node with an entropy of zero will be considered as a leaf node, and those with an entropy >0 will need further splitting till they are also zero.\n",
    "\n",
    " <div align = \"center\"> <img src = \"https://d1rwhvwstyk9gu.cloudfront.net/2022/10/Entropy.png\" height = 200px> </div>\n",
    "\n",
    "### Information Gain in Decision Trees\n",
    "\n",
    "Information gain refers to how much uncertainty a split causes the dataset to lose. It is a measure of reduction in randomness and is tied to entropy. It allows us to guage how homogenous a dataset split is. Therefore, information gain is indirectly propotional to entropy. \n",
    "\n",
    "### Gini Index \n",
    "\n",
    "The Gini index essentially functions as the cost function of the decision tree splitting algorithm. The aim for using the Gini index is to reduce the amount of impurities in a split. Gini index can be given by : \n",
    "\n",
    "\n",
    "$$ Gini = 1 - \\sum_{1 = 1}^{n} (p_i)^2\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where pi is the probability of an object being identified as a specific class. \n",
    "\n",
    "If we obtain a high Gini index, it means high amounts of heterogenity in the split. This is why we aim for low Gini index scores.\n",
    "\n",
    "\n",
    "A popular decision tree algorithm CART (Classification and Regression Tree) utilizes Gini Index to create splits. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning\n",
    "\n",
    "After the full decision tree is formed, we prune some branches which are 'redundant' from the decision tree to prevent any sort of overfitting. This is done by using a training and validation split in the dataset. We first build a decision tree according to the training set and then trim the tree according to validation set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "It is finally time to implement Decision Tree algorithms into practice!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
